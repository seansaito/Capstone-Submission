{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import datetime\n",
    "\n",
    "# Load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "comp = ['comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x']\n",
    "\n",
    "rec = ['rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey']\n",
    "\n",
    "sci = ['sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space']\n",
    "\n",
    "talk = ['talk.politics.guns',\n",
    " 'talk.politics.mideast']\n",
    "\n",
    "cats = [comp, rec, sci, talk]\n",
    "\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for idx, cat in enumerate(cats):\n",
    "    data = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, \n",
    "                                remove=remove, categories=cat)\n",
    "    all_data = all_data + data.data\n",
    "    labels = np.ones(data.target.shape[0]).astype(int) * idx\n",
    "    all_labels = all_labels + list(labels)\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X = vectorizer.fit_transform(all_data)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 100 # 1st layer num features\n",
    "n_hidden_2 = 200 # 2nd layer num features\n",
    "n_hidden_3 = 200\n",
    "encoded_dim = 50\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "dims = [n_input, 5000, 5000, 5000]\n",
    "\n",
    "weights = {}\n",
    "biases = {}\n",
    "\n",
    "# Encoder weights and biases\n",
    "for i in range(len(dims) - 1):\n",
    "    weights_key = \"encoder_h\" + str(i+1)\n",
    "    biases_key = \"encoder_b\" + str(i+1)\n",
    "    weights[weights_key] = tf.Variable(tf.random_normal([dims[i], dims[i+1]]))\n",
    "    biases[biases_key] = tf.Variable(tf.random_normal([dims[i+1]]))\n",
    "    \n",
    "# Decoder weights and biases\n",
    "for i in range(len(dims) - 1):\n",
    "    weights_key = \"decoder_h\" + str(i+1)\n",
    "    biases_key = \"decoder_b\" + str(i+1)\n",
    "    weights[weights_key] = tf.Variable(tf.random_normal([dims[::-1][i], dims[::-1][i+1]]))\n",
    "    biases[biases_key] = tf.Variable(tf.random_normal([dims[::-1][i+1]]))\n",
    "\n",
    "encoder_ops = []\n",
    "\n",
    "# Encoder\n",
    "def encoder(x):\n",
    "    prev_layer = \"\"\n",
    "    for i in range(len(dims) - 1):\n",
    "        encoder_weights = \"encoder_h\" + str(i+1)\n",
    "        encoder_biases = \"encoder_b\" + str(i+1)\n",
    "        if i == 0:\n",
    "            layer = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[encoder_weights]), \n",
    "                               biases[encoder_biases]))\n",
    "            prev_layer = layer\n",
    "        else:\n",
    "            layer = tf.nn.sigmoid(tf.add(tf.matmul(prev_layer, weights[encoder_weights]), \n",
    "                               biases[encoder_biases]))\n",
    "            prev_layer = layer\n",
    "        encoder_ops.append(layer)\n",
    "    return layer\n",
    "\n",
    "# Decoder\n",
    "def decoder(x):\n",
    "    prev_layer = \"\"\n",
    "    for i in range(len(dims) - 1):\n",
    "        decoder_weights = \"decoder_h\" + str(i+1)\n",
    "        decoder_biases = \"decoder_b\" + str(i+1)\n",
    "        if i == 0:\n",
    "            layer = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[decoder_weights]), \n",
    "                               biases[decoder_biases]))\n",
    "            prev_layer = layer\n",
    "        else:\n",
    "            layer = tf.nn.sigmoid(tf.add(tf.matmul(prev_layer, weights[decoder_weights]), \n",
    "                               biases[decoder_biases]))\n",
    "            prev_layer = layer\n",
    "    return layer\n",
    "\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Groundtruths\n",
    "y_true = X\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "batch_size = 50\n",
    "display_step = 10\n",
    "\n",
    "# Loss functions\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# Training\n",
    "for epoch_i in range(training_epochs):\n",
    "    # Loop over all batches    \n",
    "    for batch_i in range(X_train.shape[0] // batch_size):\n",
    "        batch_xs = X_train[batch_i * batch_size:(batch_i + 1) * batch_size].toarray()\n",
    "        sess.run(optimizer, feed_dict={X: batch_xs})\n",
    "    if epoch_i % display_step == 0:\n",
    "        print(str(datetime.datetime.now()), epoch_i, sess.run(cost, feed_dict={X: batch_xs}))\n",
    "\n",
    "print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "temp = []\n",
    "all_ravels = []\n",
    "\n",
    "for batch_i in range(X_train.shape[0] // batch_size):\n",
    "    batch_xs = X_train[batch_i * batch_size:(batch_i + 1) * batch_size].toarray()\n",
    "    layers = [sess.run(encoder_ops[i], feed_dict={X: batch_xs}) for i in range(len(encoder_ops))]\n",
    "    ravels = [np.array([row.ravel() for row in layers[i]]) for i in range(len(encoder_ops))]\n",
    "    all_ravels.append(ravels)\n",
    "    f = np.hstack(ravels)\n",
    "    temp.append(f)\n",
    "\n",
    "combined = np.vstack(temp)\n",
    "print combined.shape\n",
    "\n",
    "zipped = zip(*all_ravels)\n",
    "\n",
    "ravels = []\n",
    "\n",
    "for layer in zipped:\n",
    "    ravels.append(np.vstack(layer))\n",
    "\n",
    "# train = X_train.toarray()[:5000]\n",
    "# layers = [sess.run(encoder_ops[i], \n",
    "#         feed_dict={X: train}) for i in range(len(encoder_ops))]\n",
    "\n",
    "# ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(encoder_ops)))\n",
    "# combined = np.hstack(ravels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(combined, y_train[:combined.shape[0]])\n",
    "print str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print knn.score(combined, y_train[:combined.shape[0]])\n",
    "print str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = knn.predict(combined)\n",
    "print str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "class_names = [\"comp\", \"rec\", \"sci\", \"talk\"]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "#     plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, str(cm[i, j])[:4],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_train[:combined.shape[0]], y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix for 20newsgroups')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ks = [5, 10, 20]\n",
    "y = y_train[:combined.shape[0]]\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for k in ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(combined, y)\n",
    "    print (\"KNN Score with k=%i\" % k,  knn.score(combined, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
