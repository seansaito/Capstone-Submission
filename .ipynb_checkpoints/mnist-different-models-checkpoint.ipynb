{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "# %%\n",
    "def autoencoder(input_shape=[None, 784],\n",
    "                n_filters=[1, 10, 10, 10],\n",
    "                filter_sizes=[3, 3, 3],\n",
    "                corruption=False):\n",
    "    # %%\n",
    "    # input to the network\n",
    "    x = tf.placeholder(\n",
    "        tf.float32, input_shape, name='x')\n",
    "\n",
    "\n",
    "    # %%\n",
    "    # ensure 2-d is converted to square tensor.\n",
    "    if len(x.get_shape()) == 2:\n",
    "        x_dim = np.sqrt(x.get_shape().as_list()[1])\n",
    "        if x_dim != int(x_dim):\n",
    "            raise ValueError('Unsupported input dimensions')\n",
    "        x_dim = int(x_dim)\n",
    "        x_tensor = tf.reshape(\n",
    "            x, [-1, x_dim, x_dim, n_filters[0]])\n",
    "    elif len(x.get_shape()) == 4:\n",
    "        x_tensor = x\n",
    "    else:\n",
    "        raise ValueError('Unsupported input dimensions')\n",
    "    current_input = x_tensor\n",
    "\n",
    "    # %%\n",
    "    # Optionally apply denoising autoencoder\n",
    "    if corruption:\n",
    "        current_input = corrupt(current_input)\n",
    "\n",
    "    # %%\n",
    "    # Build the encoder\n",
    "    encoder_weights = []\n",
    "    encoder_ops = []\n",
    "    shapes = []\n",
    "    for layer_i, n_output in enumerate(n_filters[1:]):\n",
    "        n_input = current_input.get_shape().as_list()[3]\n",
    "        shapes.append(current_input.get_shape().as_list())\n",
    "        W = tf.Variable(\n",
    "            tf.random_uniform([\n",
    "                filter_sizes[layer_i],\n",
    "                filter_sizes[layer_i],\n",
    "                n_input, n_output],\n",
    "                -1.0 / math.sqrt(n_input),\n",
    "                1.0 / math.sqrt(n_input)))\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        encoder_weights.append(W)\n",
    "        output = lrelu(\n",
    "            tf.add(tf.nn.conv2d(\n",
    "                current_input, W, strides=[1, 1, 1, 1], padding='SAME'), b))\n",
    "        encoder_ops.append(output)\n",
    "        current_input = output\n",
    "\n",
    "    # %%\n",
    "    # store the latent representation\n",
    "    z = current_input\n",
    "    encoder_weights.reverse()\n",
    "    shapes.reverse()\n",
    "\n",
    "    # %%\n",
    "    # Build the decoder using the same weights\n",
    "    for layer_i, shape in enumerate(shapes):\n",
    "        W = encoder_weights[layer_i]\n",
    "        b = tf.Variable(tf.zeros([W.get_shape().as_list()[2]]))\n",
    "        output = lrelu(tf.add(\n",
    "            tf.nn.conv2d_transpose(\n",
    "                current_input, W,\n",
    "                tf.pack([tf.shape(x)[0], shape[1], shape[2], shape[3]]),\n",
    "                strides=[1, 1, 1, 1], padding='SAME'), b))\n",
    "        current_input = output\n",
    "\n",
    "    # %%\n",
    "    # now have the reconstruction through the network\n",
    "    y = current_input\n",
    "    # cost function measures pixel-wise difference\n",
    "    cost = tf.reduce_sum(tf.square(y - x_tensor))\n",
    "\n",
    "    # %%\n",
    "    return {'x': x, 'z': z, 'y': y, 'cost': cost, \"encoder\": encoder_ops}\n",
    "\n",
    "\n",
    "# %%\n",
    "def test_mnist(n_filters, filter_sizes):\n",
    "    \"\"\"Test the convolutional autoencder using MNIST.\"\"\"\n",
    "    # %%\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # %%\n",
    "    # load MNIST as before\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    mean_img = np.mean(mnist.train.images, axis=0)\n",
    "    ae = autoencoder(n_filters=n_filters, filter_sizes=filter_sizes)\n",
    "\n",
    "    # %%\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(ae['cost'])\n",
    "\n",
    "    # %%\n",
    "    # We create a session to use the graph\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # %%\n",
    "    # Fit all training data\n",
    "    batch_size = 100\n",
    "    n_epochs = 100\n",
    "    step_size = 10\n",
    "    for epoch_i in range(n_epochs):\n",
    "        for batch_i in range(mnist.train.num_examples // batch_size):\n",
    "            batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "            train = np.array([img - mean_img for img in batch_xs])\n",
    "            sess.run(optimizer, feed_dict={ae['x']: train})\n",
    "        if epoch_i % step_size == 0:\n",
    "            print(str(datetime.datetime.now()), epoch_i, sess.run(ae['cost'], feed_dict={ae['x']: train}))\n",
    "\n",
    "    # %%\n",
    "    # Plot example reconstructions\n",
    "    n_examples = 10\n",
    "    test_xs, _ = mnist.test.next_batch(n_examples)\n",
    "    test_xs_norm = np.array([img - mean_img for img in test_xs])\n",
    "    recon = sess.run(ae['y'], feed_dict={ae['x']: test_xs_norm})\n",
    "    print(recon.shape)\n",
    "    fig, axs = plt.subplots(2, n_examples, figsize=(10, 2))\n",
    "    for example_i in range(n_examples):\n",
    "        axs[0][example_i].imshow(\n",
    "            np.reshape(test_xs[example_i, :], (28, 28)), cmap=\"Greys_r\")\n",
    "        axs[1][example_i].imshow(\n",
    "            np.reshape(\n",
    "                np.reshape(recon[example_i, ...], (784,)) + mean_img,\n",
    "                (28, 28)), cmap=\"Greys_r\")\n",
    "    fig.show()\n",
    "    plt.draw()\n",
    "    \n",
    "    ae[\"session\"] = sess\n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "mnist.train.next_batch(10)[0].shape\n",
    "mean_img = np.mean(mnist.train.images, axis=0)\n",
    "batch_xs, batch_ys = mnist.train.next_batch(10000)\n",
    "train = np.array([img - mean_img for img in batch_xs])\n",
    "y = [np.argmax(row) for row in batch_ys]\n",
    "y_train = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2 layers\n",
    "\n",
    "ae = test_mnist([1, 10], [3])\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(combined, y_train[:combined.shape[0]])\n",
    "print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3 layers\n",
    "\n",
    "ae = test_mnist([1, 10, 10], [3, 3])\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(combined, y_train[:combined.shape[0]])\n",
    "print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = knn.predict(combined)\n",
    "print y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "class_names = [str(i) for i in range(10)]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "#     plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, str(cm[i, j])[:4],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_train[:combined.shape[0]][:1000], y_pred[:1000])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix for MNIST')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae = test_mnist([1, 10, 10, 10], [3, 3, 3])\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 8\n",
    "n_filters = [1] + [10 for i in range(n-1)]\n",
    "filter_sizes = [3 for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "n_filters = [1] + [10 for i in range(n-1)]\n",
    "filter_sizes = [3 for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "f = 5\n",
    "n_filters = [1] + [10 for i in range(n-1)]\n",
    "filter_sizes = [f for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "f = 7\n",
    "n_filters = [1] + [10 for i in range(n-1)]\n",
    "filter_sizes = [f for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "f = 9\n",
    "n_filters = [1] + [10 for i in range(n-1)]\n",
    "filter_sizes = [f for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "f = 15\n",
    "n_filters = [1] + [10 for i in range(n-1)]\n",
    "filter_sizes = [f for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "f = 3\n",
    "n_filters = [1] + [f for i in range(n-1)]\n",
    "filter_sizes = [3 for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "f = 5\n",
    "n_filters = [1] + [f for i in range(n-1)]\n",
    "filter_sizes = [3 for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "f = 10\n",
    "n_filters = [1] + [f for i in range(n-1)]\n",
    "filter_sizes = [3 for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "f = 15\n",
    "n_filters = [1] + [f for i in range(n-1)]\n",
    "filter_sizes = [3 for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "f = 20\n",
    "n_filters = [1] + [f for i in range(n-1)]\n",
    "filter_sizes = [3 for i in range(n-1)]\n",
    "\n",
    "ae = test_mnist(n_filters, filter_sizes)\n",
    "\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "    \n",
    "combined = np.vstack((combined))\n",
    "print combined.shape\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(combined, y_train[:combined.shape[0]])\n",
    "# print knn.score(combined, y_train[:combined.shape[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
