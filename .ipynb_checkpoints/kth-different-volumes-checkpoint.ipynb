{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os, time\n",
    "import tensorflow as tf\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "height, width = 100,100\n",
    "resize = (height, width)\n",
    "\n",
    "def get_frames(fname, resize=resize, num_frames=None):\n",
    "    cap = cv2.VideoCapture(fname)\n",
    "    rets, frames = [], []\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        rets.append(ret)\n",
    "        frame = np.array(Image.fromarray(frame.astype(np.uint8)).resize(resize, \n",
    "                                    Image.ANTIALIAS)) / 255.0\n",
    "        frame = np.mean(frame, axis=2)        \n",
    "        frames.append(frame)\n",
    "        i += 1\n",
    "        if num_frames is not None and i == num_frames:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return np.array(frames)\n",
    "\n",
    "label_hash = {\n",
    "    0: \"boxing\", 1: \"handclapping\", 2: \"handwaving\", 3: \"jogging\", 4: \"running\",\n",
    "    5: \"walking\"\n",
    "}\n",
    "\n",
    "reverse_labels = {val:key for key, val in label_hash.items()}\n",
    "\n",
    "num_labels = 6\n",
    "\n",
    "# group_adjacent = lambda a, k: zip(*([iter(a)] * k))                \n",
    "from itertools import islice\n",
    "group_adjacent = lambda a, k: zip(*(islice(a, i, None, k) for i in range(k)))\n",
    "\n",
    "video_files = []\n",
    "for i in range(num_labels):\n",
    "    video_files = video_files + os.listdir(label_hash[i])\n",
    "\n",
    "random.shuffle(video_files)\n",
    "\n",
    "# video_files = [os.listdir(label_hash[i]) for i in range(num_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_frames(fname, vol_size):\n",
    "    action_name = fname.split(\"_\")[1]\n",
    "    label = reverse_labels[action_name]\n",
    "    full_path = action_name + \"/\" + fname\n",
    "    frames = get_frames(full_path)\n",
    "    return_vols, return_labels = [], []\n",
    "    grouped_frames = group_adjacent(frames, vol_size)\n",
    "    for group in grouped_frames:\n",
    "        vol = np.stack((group)).transpose((1, 2, 0))\n",
    "        return_vols.append(vol)\n",
    "        return_labels.append(label)\n",
    "    final_vols = np.stack((return_vols))\n",
    "    return final_vols, return_labels\n",
    "\n",
    "def get_all_volumes_and_labels(f_store, vol_size=10):\n",
    "    all_vols, all_labels = [], []\n",
    "    print \"%i videos in total\" % len(f_store)\n",
    "    for idx, f in enumerate(f_store):\n",
    "        vols, labels = group_frames(f, vol_size)\n",
    "        all_vols.append(vols)\n",
    "        all_labels = all_labels + labels\n",
    "        print \"Video %i, added %i sequences\" % (idx, len(vols))\n",
    "    volumes = np.vstack((all_vols))\n",
    "    labels = np.array(all_labels)\n",
    "    return volumes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vol_size = 5\n",
    "train, labels = get_all_volumes_and_labels(video_files, vol_size=vol_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vol = vol_size\n",
    "\n",
    "def autoencoder(input_shape=[None, height, width, vol],\n",
    "                n_filters=[3, 30, 50, 50],\n",
    "                filter_sizes=[5, 10, 10, 10],\n",
    "                corruption=False):\n",
    "    # input to the network\n",
    "    x = tf.placeholder(tf.float32, input_shape, name='x')\n",
    "\n",
    "    # ensure 2-d is converted to square tensor.\n",
    "    if len(x.get_shape()) == 2:\n",
    "        x_dim = np.sqrt(x.get_shape().as_list()[1])\n",
    "        if x_dim != int(x_dim):\n",
    "            raise ValueError('Unsupported input dimensions')\n",
    "        x_dim = int(x_dim)\n",
    "        x_tensor = tf.reshape(\n",
    "            x, [-1, x_dim, x_dim, n_filters[0]])\n",
    "    elif len(x.get_shape()) == 4:\n",
    "        x_tensor = x\n",
    "    else:\n",
    "        raise ValueError('Unsupported input dimensions')\n",
    "    current_input = x_tensor\n",
    "\n",
    "    # Build the encoder\n",
    "    encoder_weights = []\n",
    "    encoder_ops = []\n",
    "    shapes = []\n",
    "    for layer_i, n_output in enumerate(n_filters[1:]):\n",
    "        n_input = current_input.get_shape().as_list()[3]\n",
    "        shapes.append(current_input.get_shape().as_list())\n",
    "        W = tf.Variable(\n",
    "            tf.random_uniform([\n",
    "                filter_sizes[layer_i],\n",
    "                filter_sizes[layer_i],\n",
    "                n_input, n_output],\n",
    "                -1.0 / math.sqrt(n_input),\n",
    "                1.0 / math.sqrt(n_input)))\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        encoder_weights.append(W)\n",
    "        output = tf.nn.sigmoid(\n",
    "            tf.add(tf.nn.conv2d(\n",
    "                current_input, W, strides=[1, 1, 1, 1], padding='SAME'), b))\n",
    "        encoder_ops.append(output)\n",
    "        current_input = output\n",
    "\n",
    "    # store the latent representation\n",
    "    z = current_input\n",
    "    encoder_weights.reverse()\n",
    "    shapes.reverse()\n",
    "\n",
    "    # Build the decoder using the same weights\n",
    "    for layer_i, shape in enumerate(shapes):\n",
    "        W = encoder_weights[layer_i]\n",
    "        b = tf.Variable(tf.zeros([W.get_shape().as_list()[2]]))\n",
    "        output = tf.nn.sigmoid(\n",
    "            tf.add(tf.nn.conv2d_transpose(\n",
    "                current_input, W,\n",
    "                tf.pack([tf.shape(x)[0], shape[1], shape[2], shape[3]]),\n",
    "                strides=[1, 1, 1, 1], padding='SAME'), b))\n",
    "        current_input = output\n",
    "        \n",
    "    decoder = current_input\n",
    "\n",
    "    # now have the reconstruction through the network\n",
    "    y = current_input\n",
    "    # cost function measures pixel-wise difference\n",
    "    cost = tf.reduce_sum(tf.square(y - x_tensor))\n",
    "\n",
    "    return {'x': x, 'z': z, 'y': y, 'cost': cost, \n",
    "            \"encoder\": encoder_ops, \"decoder\": decoder}\n",
    "\n",
    "\n",
    "def test_kth(X_train, X_test, n_filters, filter_sizes):\n",
    "    import tensorflow as tf\n",
    "    ae = autoencoder(n_filters=n_filters, filter_sizes=filter_sizes)\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(ae['cost'])\n",
    "\n",
    "    # We create a session to use the graph\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Fit all training data\n",
    "    batch_size = 100\n",
    "    n_epochs = 100\n",
    "    step_size = 10\n",
    "    for epoch_i in range(n_epochs):\n",
    "        for batch_i in range(X_train.shape[0] // batch_size):\n",
    "            batch_xs = X_train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "            train = batch_xs\n",
    "            sess.run(optimizer, feed_dict={ae['x']: train})\n",
    "        if epoch_i % step_size == 0:\n",
    "            print(str(datetime.datetime.now()), epoch_i, sess.run(ae['cost'], feed_dict={ae['x']: train}))\n",
    "\n",
    "    ae[\"session\"] = sess\n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae = test_kth(X_train, X_test,\n",
    "                n_filters=[vol, 5, 5, 5],\n",
    "                filter_sizes=[3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = X_train\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)\n",
    "\n",
    "del sess, ae, X_train, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined = np.vstack((combined))\n",
    "print combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined = combined[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale and visualize the embedding vectors\n",
    "def plot_embedding(X, y, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 12})\n",
    "\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "vectorized_imgs = combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "print knn.fit(combined, y_train[:combined.shape[0]])\n",
    "print str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print knn.score(combined, y_train[:combined.shape[0]])\n",
    "print str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
