{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack multiple frames to create a volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "video_files = os.listdir(\"videoclips\")\n",
    "annotation_files = os.listdir(\"annotations\")\n",
    "\n",
    "video_files.remove(\"example.avi\")\n",
    "\n",
    "def log(string):\n",
    "    print \"%s: %s\" % (str(datetime.datetime.now())[:-6], string)\n",
    "    \n",
    "proportion = 1.0\n",
    "\n",
    "with open(\"annotations/train_clean.txt\") as fp:\n",
    "    train_clean = fp.readlines()\n",
    "    train_clean = [f[:-2] for f in train_clean]\n",
    "    print len(train_clean)\n",
    "    train_clean = np.random.choice(train_clean, int(len(train_clean) * proportion))\n",
    "\n",
    "with open(\"annotations/test_clean.txt\") as fp:\n",
    "    test_clean = fp.readlines()\n",
    "    test_clean = [f[:-2] for f in test_clean]\n",
    "    test_clean = np.random.choice(test_clean, int(len(test_clean) * proportion))\n",
    "    \n",
    "with open(\"annotations/train_auto.txt\") as fp:\n",
    "    train_auto = fp.readlines()\n",
    "    train_auto = [f[:-2] for f in train_auto]\n",
    "    train_auto = np.random.choice(train_auto, int(len(train_auto) * proportion))\n",
    "    \n",
    "# def get_frames(fname, resize=(128, 128), trim=0.2, grey=False, show=False):\n",
    "def get_frames(fname, resize=(100, 100), trim=None, grey=False, show=False):\n",
    "    \"\"\"\n",
    "    Get all frames from a video file\n",
    "    Also have option to trim the number of frames\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(fname)\n",
    "    frames = []\n",
    "    originals = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = frame / 255.\n",
    "        originals.append(frame)\n",
    "        if resize is not None:\n",
    "#             frame = np.array(Image.fromarray(np.array(frame)).resize(resize, PIL.Image.ANTIALIAS))\n",
    "#             frame = np.resize(frame, resize)\n",
    "#             frame = imresize(frame, resize)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "        if grey:\n",
    "            frame = np.mean(frame, axis=2)        \n",
    "        frames.append(frame)\n",
    "\n",
    "    if show:\n",
    "        f1 = frames[1]\n",
    "        cv_rgb = f1\n",
    "        log(\"Shape: \", cv_rgb.shape)\n",
    "        plt.imshow(cv_rgb)\n",
    "        plt.show()\n",
    "\n",
    "    log(\"Number of frames: %i\" % len(frames))\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if trim is not None:\n",
    "        # Trim should < 0.5\n",
    "        trim = int(len(frames) * trim)\n",
    "        frames = frames[trim:-trim]\n",
    "        log(\"Trimmed to %i frames\" % len(frames))\n",
    "    \n",
    "    return frames, originals\n",
    "\n",
    "video_to_frame_to_label = {fname: {} for fname in video_files}\n",
    "# stores = [train_clean, test_clean, train_auto]\n",
    "stores = [train_clean, test_clean]\n",
    "\n",
    "label_bank = {\"None\": 0, \"SitUp\": 1, \"GetOutCar\": 2, \"StandUp\": 3, \"AnswerPhone\": 4,\n",
    "             \"Kiss\": 5, \"HugPerson\": 6, \"HandShake\": 7, \"SitDown\": 8}\n",
    "\n",
    "for idx, store in enumerate(stores):\n",
    "    for line in store:\n",
    "        video_title = line.split(\"\\\"\")[1]\n",
    "        begin, end = map(int, line.split(\"(\")[1].split(\")\")[0].split(\"-\"))\n",
    "        labels = []\n",
    "        label_group = line.split(\"<\")[1:]\n",
    "        for l in label_group:\n",
    "            label = l.split(\">\")[0]\n",
    "            labels.append(label_bank[label])\n",
    "        for i in range(begin - 1, end + 1):\n",
    "            if i in video_to_frame_to_label[video_title]:\n",
    "                video_to_frame_to_label[video_title][i] = \\\n",
    "                        video_to_frame_to_label[video_title][i] + labels                           \n",
    "            else:\n",
    "                video_to_frame_to_label[video_title][i] = labels\n",
    "\n",
    "# group_adjacent = lambda a, k: zip(*([iter(a)] * k))                \n",
    "from itertools import islice\n",
    "group_adjacent = lambda a, k: zip(*(islice(a, i, None, k) for i in range(k)))\n",
    "from scipy import stats\n",
    "\n",
    "def group_frames(fname, volume_size):\n",
    "    frames, originals = get_frames(fname, trim=None, grey=True)\n",
    "    if len(frames) < 1:\n",
    "        return None, None\n",
    "    video_title = fname.split(\"/\")[1]\n",
    "    return_vols = []\n",
    "    return_labels = []\n",
    "    return_originals = []\n",
    "    grouped_frames = group_adjacent(frames, volume_size)\n",
    "    for idx, group in enumerate(grouped_frames):\n",
    "        absolute_pos = idx * volume_size\n",
    "        vol = np.stack((group)).transpose((1, 2, 0))\n",
    "        if absolute_pos in video_to_frame_to_label[video_title]:\n",
    "            labels = video_to_frame_to_label[video_title][absolute_pos]\n",
    "            print \"Frame\", absolute_pos, \" labels\", labels\n",
    "            return_labels.append(stats.mode(labels)[0][0])\n",
    "            return_vols.append(vol)\n",
    "        else:\n",
    "            return_vols.append(vol)\n",
    "            return_labels.append(0)\n",
    "    if len(return_vols) == 0:\n",
    "        return None, None\n",
    "    final_vols = np.stack((return_vols))\n",
    "    return final_vols, return_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.concatenate((train_clean, test_clean)).shape\n",
    "\n",
    "train_and_test_files = train_clean = np.concatenate((train_clean, test_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_clean[-2]\n",
    "print train_clean[-2].split(\"\\\"\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_f = \"videoclips/\" + train_clean[-2].split(\"\\\"\")[1]\n",
    "frames, originals = get_frames(sample_f, resize=(100, 100), trim=None, grey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video = \"videoclips/\" + train_clean[-2].split(\"\\\"\")[1]\n",
    "vols, labels = group_frames(video, 50)\n",
    "print vols.shape\n",
    "print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(video_to_frame_to_label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_volumes_and_labels(f_store, vol_size=20):\n",
    "    all_vols, all_labels = [], []\n",
    "    for f in f_store:\n",
    "        video = \"videoclips/\" + f.split(\"\\\"\")[1]\n",
    "        vols, labels = group_frames(video, vol_size)\n",
    "        if vols is not None:\n",
    "            all_vols.append(vols)\n",
    "            all_labels = all_labels + labels\n",
    "    return all_vols, all_labels\n",
    "\n",
    "vol = 50\n",
    "train_frames, train_labels = get_all_volumes_and_labels(train_and_test_files, vol_size=vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Let's try to make an LMDB out of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import caffe\n",
    "import lmdb\n",
    "\n",
    "X_train = np.vstack((train_frames))\n",
    "y_train = np.array(train_labels)\n",
    "print X_train.shape\n",
    "print len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_frames, test_labels = get_all_volumes_and_labels(test_clean)\n",
    "# X_test = np.vstack((test_frames))\n",
    "# y_test = np.array(test_labels)\n",
    "# print X_test.shape\n",
    "# print len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "# from keras.models import Model\n",
    "\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth=True\n",
    "# sess = tf.Session(config=config)\n",
    "\n",
    "# from keras import backend as K\n",
    "# K.set_session(sess)\n",
    "\n",
    "# height, width = 128, 128\n",
    "# vol = 20\n",
    "\n",
    "# input_img = Input(shape=(height, width, vol))\n",
    "\n",
    "# # input_img = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"x\")\n",
    "\n",
    "# encoding_layers = []\n",
    "# kernel_sizes = [3, 3, 3, 3, 3]\n",
    "# num_filters = [10, 10, 10, 10, 10]\n",
    "\n",
    "# kernel_sizes = [5, 5, 5, 3, 3]\n",
    "# num_filters = [32, 32, 64, 10, 10]\n",
    "\n",
    "# kernel_sizes = [5, 5, 5, 5, 5]\n",
    "# num_filters = [48, 48, 48, 48, 48]\n",
    "\n",
    "# kernel_sizes = [3 for i in range(3)]\n",
    "# num_filters = [vol for i in range(3)]\n",
    "\n",
    "# encoded = []\n",
    "# for idx, (kernel, f) in enumerate(zip(kernel_sizes, num_filters)):\n",
    "#     if idx == 0:\n",
    "#         x = Convolution2D(f, kernel, kernel, activation=\"sigmoid\", border_mode=\"same\")(input_img)\n",
    "#     else:\n",
    "#         x = Convolution2D(f, kernel, kernel, activation=\"sigmoid\", border_mode=\"same\")(x)\n",
    "#     x = MaxPooling2D((2, 2), border_mode=\"same\")(x)\n",
    "#     encoding_layers.append(x)\n",
    "#     encoded = x\n",
    "\n",
    "# prev = []\n",
    "# for idx, (kernel, f) in enumerate(zip(kernel_sizes[::-1], num_filters[::-1])):\n",
    "#     if idx == 0:\n",
    "#         x = Convolution2D(f, kernel, kernel, activation=\"sigmoid\", border_mode=\"same\")(encoded)\n",
    "#     else:\n",
    "#         x = Convolution2D(f, kernel, kernel, activation=\"sigmoid\", border_mode=\"same\")(x)\n",
    "#     x = UpSampling2D((2, 2))(x)\n",
    "#     prev = x\n",
    "\n",
    "# decoded = Convolution2D(vol, 3, 3, activation=\"sigmoid\", border_mode=\"same\")(prev)\n",
    "\n",
    "\n",
    "\n",
    "# encoder = Model(input=input_img, output=encoded)\n",
    "\n",
    "# autoencoder = Model(input_img, decoded)\n",
    "# autoencoder.compile(optimizer='adadelta', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layers = []\n",
    "\n",
    "# with sess.as_default():\n",
    "#     autoencoder.fit(np.array(X_train), np.array(X_train), nb_epoch=50,\n",
    "#                batch_size=100, shuffle=True, \n",
    "#                 validation_data=(np.array(X_train), np.array(X_train)),\n",
    "#                verbose=0)\n",
    "#     testing = X_train\n",
    "#     layers = [layer.eval(feed_dict={input_img: testing}) \n",
    "#                   for layer in encoding_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = width = 100\n",
    "\n",
    "def autoencoder(input_shape=[None, height, width, vol],\n",
    "                n_filters=[3, 30, 50, 50],\n",
    "                filter_sizes=[5, 10, 10, 10],\n",
    "                corruption=False):\n",
    "    # input to the network\n",
    "    x = tf.placeholder(tf.float32, input_shape, name='x')\n",
    "\n",
    "    # ensure 2-d is converted to square tensor.\n",
    "    if len(x.get_shape()) == 2:\n",
    "        x_dim = np.sqrt(x.get_shape().as_list()[1])\n",
    "        if x_dim != int(x_dim):\n",
    "            raise ValueError('Unsupported input dimensions')\n",
    "        x_dim = int(x_dim)\n",
    "        x_tensor = tf.reshape(\n",
    "            x, [-1, x_dim, x_dim, n_filters[0]])\n",
    "    elif len(x.get_shape()) == 4:\n",
    "        x_tensor = x\n",
    "    else:\n",
    "        raise ValueError('Unsupported input dimensions')\n",
    "    current_input = x_tensor\n",
    "\n",
    "    # Build the encoder\n",
    "    encoder_weights = []\n",
    "    encoder_ops = []\n",
    "    shapes = []\n",
    "    for layer_i, n_output in enumerate(n_filters[1:]):\n",
    "        n_input = current_input.get_shape().as_list()[3]\n",
    "        shapes.append(current_input.get_shape().as_list())\n",
    "        W = tf.Variable(\n",
    "            tf.random_uniform([\n",
    "                filter_sizes[layer_i],\n",
    "                filter_sizes[layer_i],\n",
    "                n_input, n_output],\n",
    "                -1.0 / math.sqrt(n_input),\n",
    "                1.0 / math.sqrt(n_input)))\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        encoder_weights.append(W)\n",
    "        output = tf.nn.sigmoid(\n",
    "            tf.add(tf.nn.conv2d(\n",
    "                current_input, W, strides=[1, 1, 1, 1], padding='SAME'), b))\n",
    "        encoder_ops.append(output)\n",
    "        current_input = output\n",
    "\n",
    "    # store the latent representation\n",
    "    z = current_input\n",
    "    encoder_weights.reverse()\n",
    "    shapes.reverse()\n",
    "\n",
    "    # Build the decoder using the same weights\n",
    "    for layer_i, shape in enumerate(shapes):\n",
    "        W = encoder_weights[layer_i]\n",
    "        b = tf.Variable(tf.zeros([W.get_shape().as_list()[2]]))\n",
    "        output = tf.nn.sigmoid(\n",
    "            tf.add(tf.nn.conv2d_transpose(\n",
    "                current_input, W,\n",
    "                tf.pack([tf.shape(x)[0], shape[1], shape[2], shape[3]]),\n",
    "                strides=[1, 1, 1, 1], padding='SAME'), b))\n",
    "        current_input = output\n",
    "        \n",
    "    decoder = current_input\n",
    "\n",
    "    # now have the reconstruction through the network\n",
    "    y = current_input\n",
    "    # cost function measures pixel-wise difference\n",
    "    cost = tf.reduce_sum(tf.square(y - x_tensor))\n",
    "\n",
    "    return {'x': x, 'z': z, 'y': y, 'cost': cost, \n",
    "            \"encoder\": encoder_ops, \"decoder\": decoder}\n",
    "\n",
    "\n",
    "def test_hollywood(X_train, X_test, n_filters, filter_sizes):\n",
    "    import tensorflow as tf\n",
    "    ae = autoencoder(n_filters=n_filters, filter_sizes=filter_sizes)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(ae['cost'])\n",
    "\n",
    "    # We create a session to use the graph\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Fit all training data\n",
    "    batch_size = 100\n",
    "    n_epochs = 100\n",
    "    step_size = 10\n",
    "    for epoch_i in range(n_epochs):\n",
    "        for batch_i in range(X_train.shape[0] // batch_size):\n",
    "            batch_xs = X_train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "            train = batch_xs\n",
    "            sess.run(optimizer, feed_dict={ae['x']: train})\n",
    "        if epoch_i % step_size == 0:\n",
    "            print(str(datetime.datetime.now()), epoch_i, sess.run(ae['cost'], feed_dict={ae['x']: train}))\n",
    "\n",
    "    ae[\"session\"] = sess\n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae = test_hollywood(X_train, X_train,\n",
    "                n_filters=[vol, 20, 20, 20],\n",
    "                filter_sizes=[3, 3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = X_train\n",
    "combined = []\n",
    "sess = ae[\"session\"]\n",
    "batch_size = 100\n",
    "for batch_i in range(train.shape[0] // batch_size):\n",
    "    batch_xs = train[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "    layers = [sess.run(ae[\"encoder\"][i], \n",
    "            feed_dict={ae['x']: batch_xs}) for i in range(len(ae[\"encoder\"]))]\n",
    "    ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(ae[\"encoder\"])))\n",
    "    interm = np.hstack((ravels))\n",
    "    combined.append(interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ravels = (np.array([row.ravel() for row in layers[i]]) for i in range(len(encoding_layers)))\n",
    "combined = np.vstack((combined))\n",
    "\n",
    "del ae, sess\n",
    "\n",
    "# Scale and visualize the embedding vectors\n",
    "def plot_embedding(X, y, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 12})\n",
    "\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "vectorized_imgs = combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=0)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# X_tsne = tsne.fit_transform(vectorized_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_embedding(X_tsne, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(X_tsne, y_train[:1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# knn.score(X_tsne, y_train[:1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn.score(combined, y_train[:combined.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train[:combined.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import random\n",
    "\n",
    "# test_idxes = random.sample(range(combined.shape[0]), 10)\n",
    "test_idxes = random.sample(np.where(y_train == 0)[0], 10)\n",
    "print test_idxes\n",
    "\n",
    "tree = cKDTree(combined)\n",
    "test_array = combined[test_idxes]\n",
    "query_res = tree.query(test_array, k=10)\n",
    "\n",
    "nns = []\n",
    "\n",
    "for idx, row in enumerate(query_res[1]):\n",
    "    nn = X_train[row.ravel()]\n",
    "#     to_plot = np.vstack([X_test[idx], nn])\n",
    "    nns.append(nn)\n",
    "    \n",
    "nns = np.stack(nns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_nearest_neighbors(nns, reshape=(100, 100), cols=5, reverse=False):\n",
    "    fig, axs = plt.subplots(nns.shape[0], cols, figsize=(32, 32))\n",
    "    \n",
    "    for i in range(nns.shape[0]):\n",
    "        for j in range(cols):\n",
    "            neighbor_index = -1 * j if reverse else j\n",
    "            if len(reshape) != 2:\n",
    "                axs[i][j].imshow(\n",
    "                    nns[i, neighbor_index][:,:,0].reshape(reshape))\n",
    "            else:\n",
    "                axs[i][j].imshow(\n",
    "                    nns[i, neighbor_index][:,:,0].reshape(reshape), cmap=\"Greys_r\")\n",
    "            axs[i][j].axis(\"off\")\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_nearest_neighbors(nns, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_nearest_neighbors(nns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_nn_volumes(nns, reshape=(100, 100), row_examples=2, cols=5, reverse=False):\n",
    "    vol_size = cols\n",
    "    skip = 7\n",
    "    fig, axs = plt.subplots(row_examples * cols, vol_size, figsize=(32, 32))\n",
    "    \n",
    "    for i in range(row_examples):\n",
    "        for j in range(cols):\n",
    "            neighbor_index = -1 * j if reverse else j\n",
    "            for k in range(vol_size): # Number of frames in the volume we want to see\n",
    "                if len(reshape) != 2:\n",
    "                    axs[i * cols + j][k].imshow(\n",
    "                        nns[i, neighbor_index][:,:,k * skip].reshape(reshape))\n",
    "                else:\n",
    "                    axs[i * cols + j][k].imshow(\n",
    "                        nns[i, neighbor_index][:,:,k * skip].reshape(reshape), cmap=\"Greys_r\")\n",
    "                axs[i * cols + j][k].axis(\"off\")\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_nn_volumes(nns[5:], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# just knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_ravelled = np.array([row.ravel() for row in X_train])\n",
    "X_train = X_train_ravelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just k means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_train = X_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "def cluster_acc(Y_pred, Y):\n",
    "    \"\"\"\n",
    "    Finds the cluster accuracy\n",
    "    \"\"\"\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    Y_pred = np.array(Y_pred)\n",
    "    Y = np.array(Y)\n",
    "    D = max(Y_pred.max(), Y.max())+1\n",
    "    w = np.zeros((D,D), dtype=np.int64)\n",
    "    for i in xrange(Y_pred.size):\n",
    "        w[Y_pred[i], Y[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i,j] for i,j in ind])*1.0/Y_pred.size, w\n",
    "\n",
    "y_pred = kmeans.predict(X_train)\n",
    "cluster_acc(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_tsne, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn.score(X_tsne, y_train[:X_tsne.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
